import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import pandas as pd
import time
import re
import json

st.set_page_config(page_title="MODEL PERFORMANCE EVALUATOR", page_icon="üìä", layout="wide")
st.title("üìä Model Performance Evaluator")
st.subheader("Find the optimal model through parameter tuning")

# ==== Evaluation prompts definition ====
EVALUATION_PROMPTS = {
    "ÌïúÍµ≠Ïñ¥ Îä•Î†•": [
        "ÌïúÍµ≠Ïùò Ï†ÑÌÜµ ÏùåÏãù 3Í∞ÄÏßÄÎ•º ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.",
        "Ï°¥ÎåìÎßêÍ≥º Î∞òÎßêÏùò Ï∞®Ïù¥Ï†êÏùÑ ÏòàÏãúÏôÄ Ìï®Íªò ÏÑ§Î™ÖÌïòÏÑ∏Ïöî."
    ],
    "ÎÖºÎ¶¨Ï†Å Ï∂îÎ°†": [
        "Î™®Îì† ÏÉàÎäî ÎÇ† Ïàò ÏûàÎã§. Ìé≠Í∑ÑÏùÄ ÏÉàÎã§. Ìé≠Í∑ÑÏùÄ ÎÇ† Ïàò ÏûàÎäîÍ∞Ä? ÎÖºÎ¶¨Ï†ÅÏúºÎ°ú ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.",
        "1, 1, 2, 3, 5, 8, ? Îã§Ïùå Ïà´ÏûêÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî?"
    ],
    "Ï∞ΩÏùòÏÑ±": [
        "Ïô∏Í≥ÑÏù∏Ïù¥ ÏßÄÍµ¨Î•º Î∞©Î¨∏ÌñàÏùÑ ÎïåÏùò Ïù¥ÏïºÍ∏∞Î•º 3Î¨∏Ïû•ÏúºÎ°ú Ïç®Ï£ºÏÑ∏Ïöî.",
        "ÏãúÍ∞ÑÏó¨ÌñâÏù¥ Í∞ÄÎä•ÌïòÎã§Î©¥ Ïñ¥ÎîîÎ°ú Í∞ÄÍ≥† Ïã∂ÎÇòÏöî? Ïù¥Ïú†ÎèÑ Ìï®Íªò ÏÑ§Î™ÖÌïòÏÑ∏Ïöî."
    ],
    "Ï†ïÎ≥¥ Ï∂îÏ∂ú": [
        "Îã§Ïùå ÌÖçÏä§Ìä∏ÏóêÏÑú Ïù¥Î©îÏùº Ï£ºÏÜåÎ•º JSON Î∞∞Ïó¥Î°ú Ï∂îÏ∂úÌïòÏÑ∏Ïöî: Ïó∞ÎùΩÏ≤òÎäî john@example.comÏù¥Í≥† support@company.co.krÏûÖÎãàÎã§.",
        "Îã§Ïùå Î¨∏Ïû•ÏóêÏÑú ÎÇ†ÏßúÎ•º Ï∞æÏïÑÏ£ºÏÑ∏Ïöî: Ïö∞Î¶¨Îäî 2024ÎÖÑ 12Ïõî 25ÏùºÏóê ÎßåÎÇ† ÏòàÏ†ïÏûÖÎãàÎã§."
    ],
    "ÏàòÌïô/Í≥ÑÏÇ∞": [
        "25 √ó 37ÏùÑ Í≥ÑÏÇ∞Ìï¥Ï£ºÏÑ∏Ïöî.",
        "Îã§Ïùå Î∞©Ï†ïÏãùÏùÑ ÌíÄÏñ¥Ï£ºÏÑ∏Ïöî: 2x + 5 = 13"
    ]
}

# ==== Sidebar settings ====
with st.sidebar:
    st.header("üîß Model Settings")
    MODEL_NAME = st.selectbox(
        "MODEL",
        [
            "yanolja/EEVE-Korean-2.8B-v1.0",
            "nlpai-lab/KULLM3",
            "EleutherAI/polyglot-ko-5.8b",
        ],
        index=0
    )

    precision = st.selectbox(
        "Precision",
        ["4bit", "8bit", "fp16", "fp32"],
        index=0,
        help="Use 4bit/8bit for limited memory, fp16/fp32 if sufficient"
    )

    st.header("üéõÔ∏è Generation Parameters")
    st.info("**Note**: These parameters directly affect model performance!")

    temperature = st.slider("Temperature", 0.1, 2.0, 0.7, 0.1,
                           help="Creativity control (low=consistent, high=creative)")
    top_p = st.slider("Top-p", 0.1, 1.0, 0.9, 0.1,
                     help="Diversity control (low=conservative, high=diverse)")
    max_new_tokens = st.slider("Max Tokens", 50, 500, 256, 10,
                              help="Maximum response length")
    repetition_penalty = st.slider("Repetition Penalty", 1.0, 2.0, 1.10, 0.05,
                                  help="Reduce repetition (>1 recommended)")

    st.write("### Current Parameter Settings")
    st.code(
        f"Model: {MODEL_NAME}\n"
        f"Precision: {precision}\n"
        f"Temperature: {temperature}\n"
        f"Top-p: {top_p}\n"
        f"Max Tokens: {max_new_tokens}\n"
        f"Repetition Penalty: {repetition_penalty}"
    )

    st.markdown("---")
    st.header("üß™ Performance Evaluation")

    # ‚úÖ Baseline / Sweep switch
    baseline_mode = st.checkbox(
        "Baseline mode (single model, deterministic decoding)", value=True,
        help="do_sample=False, T=0.0, top_p=1.0, rep=1.1, Max Tokens fixed"
    )

    # Baseline Max Tokens (options: 128/256)
    baseline_max_tokens = st.selectbox(
        "Baseline Max Tokens", [128, 256], index=0,
        help="128 recommended for short QA. Use 256 if needed."
    )

    eval_mode = st.checkbox("Enable evaluation mode", value=False)

    # Sweep mode toggle
    param_test_mode = st.checkbox("Parameter sweep test (RAG presets)", value=False,
                                  help="do_sample=True with RAG presets (temp/topP/repetition/max tokens=128¬∑256)")

    # (Optional) Custom prompt input
    use_benchmark = st.checkbox("Use standard benchmark", value=True,
                               help="If checked, use EVALUATION_PROMPTS below")
    if not use_benchmark:
        eval_prompts_text = st.text_area(
            "Custom prompts (one per line)",
            height=120,
            placeholder="Enter one per line.\nEx)\nExplain in Korean.\nSolve a math problem.",
            disabled=not eval_mode
        )

    run_eval = st.button("üöÄ Start Performance Test", disabled=not eval_mode)

    if st.button("üóëÔ∏è Clear Cache"):
        st.cache_resource.clear()
        st.success("Cache cleared")

# ==== Model loading ====
@st.cache_resource(show_spinner=True)
def load_model(name: str, mode: str):
    # Tokenizer
    tok = AutoTokenizer.from_pretrained(name, use_fast=True)

    # Common options (adjust numbers for your environment)
    has_cuda = torch.cuda.is_available()
    device_map = "auto" if has_cuda else "cpu"
    max_memory = None
    if has_cuda:
        # Set GPU and CPU memory limits
        max_memory = {0: "7GiB", "cpu": "32GiB"}

    offload_kwargs = dict(
        offload_folder="offload",      # Offloading folder
        offload_state_dict=True,       # Also offload state dict
    )
